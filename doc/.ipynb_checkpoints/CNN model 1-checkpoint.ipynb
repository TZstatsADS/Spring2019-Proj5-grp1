{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tianchenwang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.layers import Dense, Flatten, GlobalAveragePooling2D, Conv2D, ConvLSTM2D, Conv3D, MaxPooling2D, Dropout, \\\n",
    "    MaxPooling3D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.utils import plot_model\n",
    "import json\n",
    "\n",
    "from EmoPy.src.callback import PlotLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _FERNeuralNet(object):\n",
    "    \"\"\"\n",
    "    Interface for all FER deep neural net classes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, emotion_map):\n",
    "        self.emotion_map = emotion_map\n",
    "        self._init_model()\n",
    "\n",
    "    def _init_model(self):\n",
    "        raise NotImplementedError(\"Class %s doesn't implement _init_model()\" % self.__class__.__name__)\n",
    "\n",
    "    def fit(self, x_train, y_train):\n",
    "        raise NotImplementedError(\"Class %s doesn't implement fit()\" % self.__class__.__name__)\n",
    "\n",
    "    def fit_generator(self, generator, validation_data=None, epochs=50):\n",
    "        self.model.compile(optimizer=\"RMSProp\", loss=\"cosine_proximity\", metrics=[\"accuracy\"])\n",
    "        self.model.fit_generator(generator=generator, validation_data=validation_data, epochs=epochs,\n",
    "                                 callbacks=[ReduceLROnPlateau(), EarlyStopping(patience=3), PlotLosses()])\n",
    "\n",
    "    def predict(self, images):\n",
    "        self.model.predict(images)\n",
    "\n",
    "    def save_model_graph(self):\n",
    "        plot_model(self.model, to_file='output/model.png')\n",
    "\n",
    "    def export_model(self, model_filepath, weights_filepath, emotion_map_filepath, emotion_map):\n",
    "        self.model.save_weights(weights_filepath)\n",
    "\n",
    "        model_json_string = self.model.to_json()\n",
    "        model_json_file = open(model_filepath, 'w')\n",
    "        model_json_file.write(model_json_string)\n",
    "        model_json_file.close()\n",
    "\n",
    "        with open(emotion_map_filepath, 'w') as fp:\n",
    "            json.dump(emotion_map, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNN(_FERNeuralNet):\n",
    "    \"\"\"\n",
    "    2D Convolutional Neural Network\n",
    "    :param image_size: dimensions of input images\n",
    "    :param channels: number of image channels\n",
    "    :param emotion_map: dict of target emotion label keys with int values corresponding to the index of the emotion probability in the prediction output array\n",
    "    :param filters: number of filters/nodes per layer in CNN\n",
    "    :param kernel_size: size of sliding window for each layer of CNN\n",
    "    :param activation: name of activation function for CNN\n",
    "    :param verbose: if true, will print out extra process information\n",
    "    **Example**::\n",
    "        net = ConvolutionalNN(target_dimensions=(64,64), channels=1, target_labels=[0,1,2,3,4,5,6], time_delay=3)\n",
    "        net.fit(features, labels, validation_split=0.15)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_size, channels, emotion_map, filters=10, kernel_size=(4, 4), activation='relu',\n",
    "                 verbose=False):\n",
    "        #we define what kind of self we want, image that self = person, and image_size can be name, channels can be age```\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "        self.filters = filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.activation = activation\n",
    "        super().__init__(emotion_map)\n",
    "\n",
    "    def _init_model(self):\n",
    "        \"\"\"\n",
    "        Composes all layers of 2D CNN.\n",
    "        \"\"\"\n",
    "        model = Sequential()\n",
    "        # Sequential() is a model in Keras which is a linear stack of layers\n",
    "        model.add(Conv2D(input_shape=list(self.image_size) + [self.channels], filters=self.filters,\n",
    "                         kernel_size=self.kernel_size, activation='relu', data_format='channels_last'))\n",
    "        #input指的是需要做卷积的输入图像，其具体含义是图片高度 图片宽度 图像管道数\n",
    "        #filter 为卷积核，具体含义是[卷积核的高度，卷积核的宽度，图像通道数，卷积核个数]，要求类型与参数input相同\n",
    "        # specifying the width and height of the 2D convolution window. K kernels waiting to be applied to the image, each kernel is convolved with input volumn and the output of each convolution \n",
    "        #operation produces a 2D output,called activation map\n",
    "        #activation 函数 为relu，增加non-linear性\n",
    "        #channels last = 输入个个维度的顺序，channel lasy对应：batch，steps，channels\n",
    "        #This layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs.\n",
    "        #The image shows the differences in size of each of the network layers.\n",
    "        #Convolutional layers apply a convolution operation to the input, passing the result to the next layer\n",
    "        #The convolution emulates the response of an individual neuron to visual stimuli.\n",
    "        #Pooling layers are used to reduce the input space and thus complexity and processing time.\n",
    "        #The flattening layer converts the output of the previous layer to a one-dimensional vector,\n",
    "        #and the final layer takes that vector and calculates a final classification output.\n",
    "        model.add(\n",
    "            Conv2D(filters=self.filters, kernel_size=self.kernel_size, activation='relu', data_format='channels_last'))\n",
    "        model.add(MaxPooling2D())\n",
    "        model.add(\n",
    "            Conv2D(filters=self.filters, kernel_size=self.kernel_size, activation='relu', data_format='channels_last'))\n",
    "        model.add(\n",
    "            Conv2D(filters=self.filters, kernel_size=self.kernel_size, activation='relu', data_format='channels_last'))\n",
    "        model.add(MaxPooling2D())\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(units=len(self.emotion_map.keys()), activation=\"relu\"))\n",
    "        if self.verbose:\n",
    "            model.summary()\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, image_data, labels, validation_split, epochs=50):\n",
    "        \"\"\"\n",
    "        Trains the neural net on the data provided.\n",
    "        :param image_data: Numpy array of training data.\n",
    "        :param labels: Numpy array of target (label) data.\n",
    "        :param validation_split: Float between 0 and 1. Percentage of training data to use for validation\n",
    "        :param batch_size:\n",
    "        :param epochs: number of times to train over input dataset.\n",
    "        \"\"\"\n",
    "        self.model.compile(optimizer=\"RMSProp\", loss=\"cosine_proximity\", metrics=[\"accuracy\"])\n",
    "        self.model.fit(image_data, labels, epochs=epochs, validation_split=validation_split,\n",
    "                       callbacks=[ReduceLROnPlateau(), EarlyStopping(patience=3)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EmoPy.src.fermodel import FERModel\n",
    "from EmoPy.src.directory_data_loader import DirectoryDataLoader\n",
    "from EmoPy.src.data_generator import DataGenerator\n",
    "from pkg_resources import resource_filename,resource_exists\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from EmoPy.src.csv_data_loader import CSVDataLoader\n",
    "import cv2\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Convolutional Model -------------------\n",
      "Loading data...\n",
      "Extracting training data from csv...\n",
      "\n",
      "DATASET DETAILS\n",
      "35887 image samples\n",
      "28709 training samples\n",
      "7178 test samples\n",
      "\n",
      "Preparing training/testing data...\n"
     ]
    }
   ],
   "source": [
    "validation_split = 0.15\n",
    "\n",
    "print('--------------- Convolutional Model -------------------')\n",
    "print('Loading data...')\n",
    "# 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n",
    "raw_dimensions = (48, 48)\n",
    "target_dimensions = (64, 64)\n",
    "verbose = True\n",
    "channels = 3\n",
    "\n",
    "emotion_map = {'0':'Angry',\n",
    "              '1':'Disgust',\n",
    "              '2':'Fear',\n",
    "              '3':'Happy',\n",
    "              '4':'Sad',\n",
    "              '5':'Surprise',\n",
    "              '6':'Neutral'}\n",
    "data_loader = CSVDataLoader(emotion_map, datapath='../data/fer2013.csv',\n",
    "                            image_dimensions=raw_dimensions, \n",
    "                            csv_label_col=0, csv_image_col=1, out_channels=3)\n",
    "\n",
    "dataset = data_loader.load_data()\n",
    "\n",
    "if verbose:\n",
    "    dataset.print_data_details()\n",
    "\n",
    "print('Preparing training/testing data...')\n",
    "train_images, train_labels = dataset.get_training_data()\n",
    "train_gen = DataGenerator().fit(train_images, train_labels)\n",
    "test_images, test_labels = dataset.get_test_data()\n",
    "test_gen = DataGenerator().fit(test_images, test_labels)\n",
    "\n",
    "# print('Training net...')\n",
    "# model = ConvolutionalNN(target_dimensions, channels, dataset.get_emotion_index_map(), \n",
    "#                         filters = 50, kernel_size=(2,2), verbose=True)\n",
    "# model.fit_generator(train_gen.generate(target_dimensions, batch_size=5),\n",
    "#                     test_gen.generate(target_dimensions, batch_size=5),\n",
    "#                     epochs=5)\n",
    "\n",
    "# Save model configuration\n",
    "# model.export_model('output/conv2d_model.json','output/conv2d_weights.h5',\"output/conv2d_emotion_map.json\", emotion_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28709, 48, 48, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training model CNNLSTM\n",
    "# from EmoPy.src.neuralnets import ConvolutionalLstmNN\n",
    "\n",
    "# start = time.time()\n",
    "# model = ConvolutionalLstmNN(target_dimensions, channels, \n",
    "#                             dataset.get_emotion_index_map(), verbose=True)\n",
    "\n",
    "# model.fit_generator(train_gen.generate(target_dimensions, batch_size=5),\n",
    "#                     test_gen.generate(target_dimensions, batch_size=5),\n",
    "#                     epochs=5)\n",
    "\n",
    "# end = time.time()\n",
    "# print(end-start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
